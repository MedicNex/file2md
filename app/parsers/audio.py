import os
import asyncio
import tempfile
import subprocess
import numpy as np
from typing import List, Tuple, Optional
import httpx
from loguru import logger
from concurrent.futures import ThreadPoolExecutor

from .base import BaseParser
from app.config import config

try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False
    logger.warning("pydubæœªå®‰è£…ï¼ŒéŸ³é¢‘å¤„ç†åŠŸèƒ½å°†å—é™")

try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    logger.warning("librosaæœªå®‰è£…ï¼Œé«˜çº§éŸ³é¢‘åˆ†æåŠŸèƒ½å°†å—é™")


class AudioSegmentInfo:
    """éŸ³é¢‘ç‰‡æ®µä¿¡æ¯"""
    def __init__(self, start_time: float, end_time: float, segment, 
                 confidence: float = 1.0):
        self.start_time = start_time
        self.end_time = end_time
        self.segment = segment
        self.confidence = confidence
        self.duration = end_time - start_time


class AudioParser(BaseParser):
    """éŸ³é¢‘/è§†é¢‘æ–‡ä»¶è§£æå™¨ - æ”¯æŒåˆ†å—å¤„ç†å’ŒASRè½¬æ¢"""
    
    @classmethod
    def get_supported_extensions(cls) -> List[str]:
        return ['.wav', '.mp3', '.mp4', '.m4a', '.flac', '.ogg', '.wma', '.aac', 
                '.avi', '.mov', '.wmv', '.mkv', '.webm', '.3gp']
    
    def __init__(self):
        super().__init__()
        self.asr_model = os.getenv("ASR_MODEL", "")
        self.asr_api_base = os.getenv("ASR_API_BASE", "")
        self.asr_api_key = os.getenv("ASR_API_KEY", "")
        self.max_segment_duration = 30  # æœ€å¤§æ®µé•¿åº¦(ç§’)
        self.min_segment_duration = 2   # æœ€å°æ®µé•¿åº¦(ç§’)
        
        # è§†é¢‘æ ¼å¼åˆ—è¡¨
        self.video_extensions = {'.mp4', '.avi', '.mov', '.wmv', '.mkv', '.webm', '.3gp'}
    
    async def parse(self, file_path: str) -> str:
        """è§£æéŸ³é¢‘/è§†é¢‘æ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ–‡æœ¬"""
        try:
            if not PYDUB_AVAILABLE:
                raise Exception("éŸ³é¢‘å¤„ç†éœ€è¦å®‰è£…pydubåº“: pip install pydub")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            file_extension = os.path.splitext(file_path)[1].lower()
            is_video = file_extension in self.video_extensions
            
            if is_video:
                logger.info(f"å¼€å§‹è§£æè§†é¢‘æ–‡ä»¶: {file_path}")
            else:
                logger.info(f"å¼€å§‹è§£æéŸ³é¢‘æ–‡ä»¶: {file_path}")
            
            # 1. é¢„å¤„ç†éŸ³é¢‘ï¼ˆè§†é¢‘æ–‡ä»¶ä¼šè‡ªåŠ¨æå–éŸ³é¢‘è½¨é“ï¼‰
            audio = await self._preprocess_audio(file_path)
            
            # 2. åŸºäºèƒ½é‡åˆ†æè¿›è¡Œåˆ†å—
            segments = await self._split_audio_by_energy(audio)
            
            # 3. å¹¶å‘ASRè½¬æ¢
            transcriptions = await self._batch_transcribe_segments(segments)
            
            # 4. æ ¼å¼åŒ–è¾“å‡ºï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¾“å‡ºæ ¼å¼ï¼‰
            if is_video:
                formatted_content = await self._format_video_subtitle_output(
                    file_path, audio, segments, transcriptions
                )
            else:
                formatted_content = await self._format_audio_output(
                    file_path, audio, segments, transcriptions
                )
            
            logger.info(f"æˆåŠŸè§£æ{'è§†é¢‘' if is_video else 'éŸ³é¢‘'}æ–‡ä»¶: {file_path}, å…±{len(segments)}ä¸ªç‰‡æ®µ")
            return formatted_content
            
        except Exception as e:
            logger.error(f"è§£æ{'è§†é¢‘' if is_video else 'éŸ³é¢‘'}æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            raise Exception(f"{'è§†é¢‘' if is_video else 'éŸ³é¢‘'}æ–‡ä»¶è§£æé”™è¯¯: {str(e)}")
    
    async def _preprocess_audio(self, file_path: str):
        """éŸ³é¢‘é¢„å¤„ç† - ç»Ÿä¸€é‡‡æ ·ç‡ã€è½¬å•å£°é“ã€å»ç›´æµåç§»"""
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            audio = AudioSegment.from_file(file_path)
            
            # è½¬æ¢ä¸ºå•å£°é“
            if audio.channels > 1:
                audio = audio.set_channels(1)
                logger.info("éŸ³é¢‘è½¬æ¢ä¸ºå•å£°é“")
            
            # ç»Ÿä¸€é‡‡æ ·ç‡ä¸º16kHz (é€‚åˆè¯­éŸ³è¯†åˆ«)
            target_sample_rate = 16000
            if audio.frame_rate != target_sample_rate:
                audio = audio.set_frame_rate(target_sample_rate)
                logger.info(f"éŸ³é¢‘é‡‡æ ·ç‡è°ƒæ•´ä¸º {target_sample_rate}Hz")
            
            # å»ç›´æµåç§» (ä½¿ç”¨é«˜é€šæ»¤æ³¢)
            audio = audio.high_pass_filter(80)  # 80Hzé«˜é€šï¼Œå»é™¤ä½é¢‘å™ªå£°
            
            logger.info(f"éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {len(audio)/1000:.1f}ç§’, {audio.frame_rate}Hz")
            return audio
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    async def _split_audio_by_energy(self, audio, 
                                   frame_ms: int = 20,
                                   min_silence_len: int = 300,
                                   silence_thresh_db: Optional[float] = None,
                                   highpass_hz: int = 150) -> List[AudioSegmentInfo]:
        """åŸºäºèƒ½é‡åˆ†æçš„æ™ºèƒ½éŸ³é¢‘åˆ†å—"""
        try:
            logger.info("å¼€å§‹åŸºäºèƒ½é‡åˆ†æçš„éŸ³é¢‘åˆ†å—...")
            
            # é«˜é€šæ»¤æ³¢å»é™¤ä½é¢‘å™ªå£°
            if highpass_hz:
                filtered_audio = audio.high_pass_filter(highpass_hz)
            else:
                filtered_audio = audio
            
            # è®¡ç®—çŸ­æ—¶èƒ½é‡(RMS)
            frame_samples = int(audio.frame_rate * frame_ms / 1000)
            raw = np.array(filtered_audio.get_array_of_samples())
            
            # ç¡®ä¿èƒ½æ•´é™¤æˆå®Œæ•´å¸§
            total_frames = len(raw) // frame_samples
            raw = raw[:total_frames * frame_samples]
            frames = raw.reshape(-1, frame_samples)
            
            # è®¡ç®—RMSèƒ½é‡
            rms = np.sqrt((frames.astype(np.float32)**2).mean(axis=1))
            rms_db = 20 * np.log10(rms + 1e-10)  # é¿å…log(0)
            
            # è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—
            if silence_thresh_db is None:
                # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•: 10åˆ†ä½å€¼ + 3dBç¼“å†²
                p10 = np.percentile(rms_db, 10)
                silence_thresh_db = p10 + 3
                logger.info(f"è‡ªåŠ¨è®¡ç®—é™éŸ³é˜ˆå€¼: {silence_thresh_db:.1f} dBFS")
            
            # æ ‡è®°é™éŸ³å¸§
            silent = rms_db < silence_thresh_db
            
            # åˆå¹¶è¿ç»­é™éŸ³åŒºåŸŸ
            change = np.diff(np.r_[0, silent, 0])
            starts = np.where(change == 1)[0]
            ends = np.where(change == -1)[0]
            
            # è¿‡æ»¤çŸ­é™éŸ³ï¼Œå¹¶åˆå¹¶ç›¸è¿‘çš„é™éŸ³åŒºåŸŸ
            valid_silences = []
            for s, e in zip(starts, ends):
                duration_ms = (e - s) * frame_ms
                if duration_ms >= min_silence_len:
                    valid_silences.append((s * frame_ms, e * frame_ms))
            
            # åˆå¹¶é—´éš”å¾ˆè¿‘çš„é™éŸ³åŒºåŸŸ (< 200ms)
            merged_silences = self._merge_close_silences(valid_silences, 200)
            
            # ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ
            segments = []
            last_end = 0
            
            for silence_start, silence_end in merged_silences:
                # æ·»åŠ å½“å‰è¯­éŸ³æ®µ
                if silence_start > last_end:
                    segment_audio = audio[last_end:silence_start]
                    if len(segment_audio) >= self.min_segment_duration * 1000:
                        # è®¡ç®—ç½®ä¿¡åº¦ (åŸºäºå¹³å‡èƒ½é‡)
                        segment_frames = frames[last_end//frame_ms:silence_start//frame_ms]
                        if len(segment_frames) > 0:
                            avg_energy = np.mean(rms_db[last_end//frame_ms:silence_start//frame_ms])
                            confidence = min(1.0, max(0.1, (avg_energy - silence_thresh_db) / 20))
                        else:
                            confidence = 0.5
                        
                        segments.append(AudioSegmentInfo(
                            start_time=last_end / 1000,
                            end_time=silence_start / 1000,
                            segment=segment_audio,
                            confidence=confidence
                        ))
                
                last_end = silence_end
            
            # æ·»åŠ æœ€åä¸€æ®µ
            if last_end < len(audio):
                segment_audio = audio[last_end:]
                if len(segment_audio) >= self.min_segment_duration * 1000:
                    avg_energy = np.mean(rms_db[last_end//frame_ms:])
                    confidence = min(1.0, max(0.1, (avg_energy - silence_thresh_db) / 20))
                    
                    segments.append(AudioSegmentInfo(
                        start_time=last_end / 1000,
                        end_time=len(audio) / 1000,
                        segment=segment_audio,
                        confidence=confidence
                    ))
            
            # è¿›ä¸€æ­¥å¤„ç†è¿‡é•¿çš„ç‰‡æ®µ
            final_segments = []
            for seg in segments:
                if seg.duration > self.max_segment_duration:
                    # åˆ†å‰²è¿‡é•¿çš„æ®µ
                    sub_segments = self._split_long_segment(seg)
                    final_segments.extend(sub_segments)
                else:
                    final_segments.append(seg)
            
            logger.info(f"éŸ³é¢‘åˆ†å—å®Œæˆ: {len(final_segments)}ä¸ªç‰‡æ®µ, é˜ˆå€¼ {silence_thresh_db:.1f} dBFS")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆç‰‡æ®µï¼Œä½¿ç”¨æ—¶é—´åˆ†å‰²ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            if not final_segments:
                logger.warning("åŸºäºèƒ½é‡çš„åˆ†å—æœªå‘ç°æœ‰æ•ˆç‰‡æ®µï¼Œä½¿ç”¨æ—¶é—´åˆ†å‰²åå¤‡æ–¹æ¡ˆ")
                return self._fallback_time_split(audio)
            
            return final_segments
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘åˆ†å—å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šç®€å•æ—¶é—´åˆ†å‰²
            return self._fallback_time_split(audio)
    
    def _merge_close_silences(self, silences: List[Tuple[int, int]], 
                            merge_threshold_ms: int) -> List[Tuple[int, int]]:
        """åˆå¹¶é—´éš”å¾ˆè¿‘çš„é™éŸ³åŒºåŸŸ"""
        if not silences:
            return []
        
        merged = [silences[0]]
        for start, end in silences[1:]:
            last_end = merged[-1][1]
            if start - last_end <= merge_threshold_ms:
                # åˆå¹¶
                merged[-1] = (merged[-1][0], end)
            else:
                merged.append((start, end))
        
        return merged
    
    def _split_long_segment(self, segment: AudioSegmentInfo) -> List[AudioSegmentInfo]:
        """åˆ†å‰²è¿‡é•¿çš„éŸ³é¢‘æ®µ"""
        sub_segments = []
        current_time = segment.start_time
        segment_audio = segment.segment
        
        while current_time < segment.end_time:
            chunk_end_time = min(current_time + self.max_segment_duration, segment.end_time)
            chunk_start_ms = int((current_time - segment.start_time) * 1000)
            chunk_end_ms = int((chunk_end_time - segment.start_time) * 1000)
            
            chunk_audio = segment_audio[chunk_start_ms:chunk_end_ms]
            
            sub_segments.append(AudioSegmentInfo(
                start_time=current_time,
                end_time=chunk_end_time,
                segment=chunk_audio,
                confidence=segment.confidence
            ))
            
            current_time = chunk_end_time
        
        return sub_segments
    
    def _fallback_time_split(self, audio) -> List[AudioSegmentInfo]:
        """é™çº§æ–¹æ¡ˆï¼šç®€å•æ—¶é—´åˆ†å‰²"""
        logger.warning("ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šç®€å•æ—¶é—´åˆ†å‰²")
        segments = []
        current_time = 0
        
        while current_time < len(audio) / 1000:
            end_time = min(current_time + self.max_segment_duration, len(audio) / 1000)
            start_ms = int(current_time * 1000)
            end_ms = int(end_time * 1000)
            
            segment_audio = audio[start_ms:end_ms]
            segments.append(AudioSegmentInfo(
                start_time=current_time,
                end_time=end_time,
                segment=segment_audio,
                confidence=0.8  # ä¸­ç­‰ç½®ä¿¡åº¦
            ))
            
            current_time = end_time
        
        return segments
    
    async def _batch_transcribe_segments(self, segments: List[AudioSegmentInfo]) -> List[str]:
        """å¹¶å‘æ‰¹é‡ASRè½¬æ¢"""
        try:
            logger.info(f"å¼€å§‹å¹¶å‘ASRè½¬æ¢ {len(segments)} ä¸ªç‰‡æ®µ...")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
            max_workers = min(len(segments), config.MAX_CONCURRENT)
            
            async def transcribe_single_segment(segment: AudioSegmentInfo) -> str:
                try:
                    # ä¿å­˜ç‰‡æ®µåˆ°ä¸´æ—¶æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                        segment.segment.export(temp_file.name, format='wav')
                        temp_path = temp_file.name
                    
                    try:
                        # è°ƒç”¨ASR API
                        transcription = await self._call_asr_api(temp_path)
                        
                        # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´è¾“å‡º
                        if segment.confidence < 0.3:
                            transcription = f"[ä½è´¨é‡éŸ³é¢‘] {transcription}"
                        
                        return transcription
                        
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if os.path.exists(temp_path):
                            os.unlink(temp_path)
                            
                except Exception as e:
                    logger.warning(f"ç‰‡æ®µè½¬æ¢å¤±è´¥: {e}")
                    return f"[è½¬æ¢å¤±è´¥: {str(e)}]"
            
            # å¹¶å‘æ‰§è¡Œè½¬æ¢
            tasks = [transcribe_single_segment(seg) for seg in segments]
            transcriptions = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            results = []
            for i, result in enumerate(transcriptions):
                if isinstance(result, Exception):
                    logger.error(f"ç‰‡æ®µ {i} è½¬æ¢å¼‚å¸¸: {result}")
                    results.append(f"[è½¬æ¢å¼‚å¸¸: {str(result)}]")
                else:
                    results.append(result)
            
            logger.info(f"ASRè½¬æ¢å®Œæˆ: {len(results)}ä¸ªç‰‡æ®µ")
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ASRè½¬æ¢å¤±è´¥: {e}")
            return [f"[ASRè½¬æ¢å¤±è´¥: {str(e)}]"] * len(segments)
    
    async def _call_asr_api(self, audio_file_path: str) -> str:
        """è°ƒç”¨ASR APIè¿›è¡Œè¯­éŸ³è¯†åˆ«"""
        if not self.asr_api_key or not self.asr_api_base:
            return "[ASRæœªé…ç½®]"
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                with open(audio_file_path, 'rb') as f:
                    files = {'file': f}
                    data = {
                        'model': self.asr_model or 'whisper-1',
                        'response_format': 'text'
                    }
                    headers = {
                        'Authorization': f'Bearer {self.asr_api_key}'
                    }
                    
                    response = await client.post(
                        f"{self.asr_api_base.rstrip('/')}/audio/transcriptions",
                        files=files,
                        data=data,
                        headers=headers
                    )
                    
                    if response.status_code == 200:
                        return response.text.strip()
                    else:
                        logger.error(f"ASR APIé”™è¯¯: {response.status_code} - {response.text}")
                        return f"[ASR APIé”™è¯¯: {response.status_code}]"
                        
        except Exception as e:
            logger.error(f"ASR APIè°ƒç”¨å¤±è´¥: {e}")
            return f"[ASRè°ƒç”¨å¤±è´¥: {str(e)}]"
    
    async def _format_audio_output(self, file_path: str, audio, 
                                 segments: List[AudioSegmentInfo], 
                                 transcriptions: List[str]) -> str:
        """æ ¼å¼åŒ–éŸ³é¢‘è§£æè¾“å‡º"""
        try:
            # åŸºæœ¬ä¿¡æ¯
            duration_seconds = len(audio) / 1000
            sample_rate = audio.frame_rate
            channels = audio.channels
            
            # æ„å»ºè¾“å‡ºå†…å®¹
            output_parts = []
            
            # éŸ³é¢‘åŸºæœ¬ä¿¡æ¯
            output_parts.append("# éŸ³é¢‘ä¿¡æ¯")
            output_parts.append(f"**æ–‡ä»¶å**: {os.path.basename(file_path)}")
            output_parts.append(f"**æ—¶é•¿**: {duration_seconds:.1f} ç§’")
            output_parts.append(f"**é‡‡æ ·ç‡**: {sample_rate} Hz")
            output_parts.append(f"**å£°é“æ•°**: {channels}")
            output_parts.append(f"**ç‰‡æ®µæ•°**: {len(segments)}")
            output_parts.append("")
            
            # åˆ†æ®µè½¬å½•ç»“æœ
            output_parts.append("# è¯­éŸ³è½¬å½•")
            output_parts.append("")
            
            for i, (segment, transcription) in enumerate(zip(segments, transcriptions)):
                start_time = segment.start_time
                end_time = segment.end_time
                confidence = segment.confidence
                
                # æ—¶é—´æˆ³æ ¼å¼åŒ–
                start_str = self._format_timestamp(start_time)
                end_str = self._format_timestamp(end_time)
                
                # ç‰‡æ®µæ ‡é¢˜
                output_parts.append(f"## ç‰‡æ®µ {i+1} [{start_str} - {end_str}]")
                
                # ç½®ä¿¡åº¦æŒ‡ç¤º
                if confidence >= 0.8:
                    quality_indicator = "ğŸŸ¢ é«˜è´¨é‡"
                elif confidence >= 0.5:
                    quality_indicator = "ğŸŸ¡ ä¸­ç­‰è´¨é‡"
                else:
                    quality_indicator = "ğŸ”´ ä½è´¨é‡"
                
                output_parts.append(f"**éŸ³è´¨**: {quality_indicator} (ç½®ä¿¡åº¦: {confidence:.2f})")
                output_parts.append("")
                
                # è½¬å½•æ–‡æœ¬
                if transcription and transcription.strip():
                    output_parts.append(transcription.strip())
                else:
                    output_parts.append("*[æ— æ³•è¯†åˆ«çš„è¯­éŸ³å†…å®¹]*")
                
                output_parts.append("")
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_chars = sum(len(t) for t in transcriptions if t)
            valid_segments = sum(1 for t in transcriptions if t and not t.startswith('['))
            
            output_parts.append("# å¤„ç†ç»Ÿè®¡")
            output_parts.append(f"**æœ‰æ•ˆç‰‡æ®µ**: {valid_segments}/{len(segments)}")
            output_parts.append(f"**è½¬å½•å­—ç¬¦æ•°**: {total_chars}")
            if len(segments) > 0:
                output_parts.append(f"**å¹³å‡ç‰‡æ®µæ—¶é•¿**: {duration_seconds/len(segments):.1f} ç§’")
            else:
                output_parts.append("**å¹³å‡ç‰‡æ®µæ—¶é•¿**: N/A (æ— æœ‰æ•ˆç‰‡æ®µ)")
            
            # æ ¼å¼åŒ–ä¸ºéŸ³é¢‘ä»£ç å—
            content = '\n'.join(output_parts)
            formatted_content = f"```audio\n{content}\n```"
            
            return formatted_content
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–è¾“å‡ºå¤±è´¥: {e}")
            return f"```audio\n# éŸ³é¢‘è§£æå¤±è´¥\né”™è¯¯: {str(e)}\n```"
    
    async def _format_video_subtitle_output(self, file_path: str, audio, 
                                           segments: List[AudioSegmentInfo], 
                                           transcriptions: List[str]) -> str:
        """æ ¼å¼åŒ–è§†é¢‘å­—å¹•è¾“å‡º"""
        try:
            # åŸºæœ¬ä¿¡æ¯
            duration_seconds = len(audio) / 1000
            sample_rate = audio.frame_rate
            
            # æ„å»ºè¾“å‡ºå†…å®¹
            output_parts = []
            
            # è§†é¢‘åŸºæœ¬ä¿¡æ¯
            output_parts.append("# Video Information")
            output_parts.append(f"**Filename**: {os.path.basename(file_path)}")
            output_parts.append(f"**Audio Duration**: {duration_seconds:.1f} seconds")
            output_parts.append(f"**Sample Rate**: {sample_rate} Hz")
            output_parts.append(f"**Segments**: {len(segments)}")
            output_parts.append("")
            
            # å­—å¹•å†…å®¹
            output_parts.append("# Subtitles")
            output_parts.append("")
            
            for i, (segment, transcription) in enumerate(zip(segments, transcriptions)):
                start_time = segment.start_time
                end_time = segment.end_time
                confidence = segment.confidence
                
                # SRTæ ¼å¼çš„æ—¶é—´æˆ³
                start_str = self._format_srt_timestamp(start_time)
                end_str = self._format_srt_timestamp(end_time)
                
                # å­—å¹•æ¡ç›®ï¼ˆç±»ä¼¼SRTæ ¼å¼ï¼‰
                output_parts.append(f"{i+1}")
                output_parts.append(f"{start_str} --> {end_str}")
                
                # è½¬å½•æ–‡æœ¬ï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if transcription and transcription.strip() and not transcription.startswith('['):
                    output_parts.append(transcription.strip())
                else:
                    # å¦‚æœæ˜¯ä½è´¨é‡æˆ–å¤±è´¥çš„è½¬å½•ï¼Œæ·»åŠ è´¨é‡æ ‡è®°
                    if confidence < 0.5:
                        output_parts.append(f"[Low Quality Audio] {transcription}")
                    else:
                        output_parts.append("*[Inaudible]*")
                
                output_parts.append("")  # ç©ºè¡Œåˆ†éš”
            
            # å¤„ç†ç»Ÿè®¡
            total_chars = sum(len(t) for t in transcriptions if t and not t.startswith('['))
            valid_segments = sum(1 for t in transcriptions if t and not t.startswith('['))
            
            output_parts.append("# Processing Statistics")
            output_parts.append(f"**Valid Segments**: {valid_segments}/{len(segments)}")
            output_parts.append(f"**Total Characters**: {total_chars}")
            if len(segments) > 0:
                output_parts.append(f"**Average Segment Duration**: {duration_seconds/len(segments):.1f} seconds")
            else:
                output_parts.append("**Average Segment Duration**: N/A (no valid segments)")
            
            # æ ¼å¼åŒ–ä¸ºè§†é¢‘ä»£ç å—
            content = '\n'.join(output_parts)
            formatted_content = f"```video\n{content}\n```"
            
            return formatted_content
            
        except Exception as e:
            logger.error(f"è§†é¢‘å­—å¹•æ ¼å¼åŒ–å¤±è´¥: {e}")
            return f"```video\n# Video Subtitle Generation Failed\nError: {str(e)}\n```"
    
    def _format_srt_timestamp(self, seconds: float) -> str:
        """æ ¼å¼åŒ–SRTæ—¶é—´æˆ³ä¸º HH:MM:SS,mmm æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        milliseconds = int((seconds % 1) * 1000)
        seconds = int(seconds)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"
    
    def _format_timestamp(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸º MM:SS.mmm æ ¼å¼"""
        minutes = int(seconds // 60)
        seconds = seconds % 60
        return f"{minutes:02d}:{seconds:06.3f}" 